{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms,utils\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import PIL\n",
    "import torch.multiprocessing\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'efficientnet-b0'\n",
    "#writer = SummaryWriter(\"/p300/Result_New_Tboard_Tumor_Normal/\")\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "#writer_path = '/p300/Tboard_try_paper'\n",
    "writer_path = '/Users/weizhenliu/Downloads/Tboard_newest'\n",
    "# data_dir = '/p300/hymenoptera_data'\n",
    "data_dir = '/Users/weizhenliu/Downloads/AntsAndBees'\n",
    "BatchSize = 8\n",
    "NumWorkers = 2\n",
    "# DownsamplePCT = 0.05\n",
    "learning_rate = 0.001\n",
    "# image_size = EfficientNet.get_image_size(model_name)\n",
    "writer = SummaryWriter(writer_path)\n",
    "WeightDecay=0\n",
    "StepSize = 300\n",
    "Gamma = 0.1\n",
    "Momentum = 0.9\n",
    "Epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# data_dir = '/Users/weizhenliu/Downloads/AntsAndBees'\n",
    "image_datasets = {x: ImageFolderWithPaths(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_WSI_name_label = {}\n",
    "Val_WSI_name_label = {}\n",
    "\n",
    "class_label = image_datasets['train'].class_to_idx\n",
    "group = [d for d in os.listdir(data_dir) if not d[0] == '.']\n",
    "classes = [d for d in os.listdir(os.path.join(data_dir,group[0])) if not d[0] == '.']\n",
    "\n",
    "for c in classes:\n",
    "    for f in os.listdir(os.path.join(data_dir,\"train\",c)):\n",
    "        if str(f)[-3:] == \"jpg\" and f.split(\"_\")[0] not in Train_WSI_name_label:\n",
    "            Train_WSI_name_label[f.split(\"_\")[0]] = class_label[c]\n",
    "            \n",
    "TrainWSIlabel = [j for i,j in Train_WSI_name_label.items()]\n",
    "\n",
    "for c in classes:\n",
    "    for f in os.listdir(os.path.join(data_dir,\"val\",c)):\n",
    "        if str(f)[-3:] == \"jpg\" and f.split(\"_\")[0] not in Val_WSI_name_label:\n",
    "            Val_WSI_name_label[f.split(\"_\")[0]] = class_label[c]\n",
    "\n",
    "ValWSIlabel = [j for i,j in Val_WSI_name_label.items()]\n",
    "\n",
    "# print(Train_WSI_name_label,TrainWSIlabel)\n",
    "# print(\"---------------------------------\")\n",
    "# print(Val_WSI_name_label,ValWSIlabel)\n",
    "WSI_label = {\"train\":TrainWSIlabel,\"val\":ValWSIlabel}\n",
    "WSI_name_label = {\"train\":Train_WSI_name_label,\"val\":Val_WSI_name_label}\n",
    "# print(WSI_name_label[\"train\"])\n",
    "# print(\"----------\")\n",
    "# print(WSI_name_label[\"val\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WSI_preds = [[0 for i in range(2)] for i in range(len(WSI_label[phase]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(WSI_label[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            WSI_preds = [[0 for i in range(2)] for i in range(len(WSI_label[phase]))]\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, paths in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                WSI_name = [os.path.basename(path).split(\"_\")[0] for path in paths]\n",
    "#                 print(\"WSI_name:\",WSI_name)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    pred = [p.item() for p in preds]\n",
    "#                     print('pred:',pred)\n",
    "                    for i,j in zip(WSI_name,pred):\n",
    "                        WSI_preds[list(WSI_name_label[phase].keys()).index(i)][j] += 1\n",
    "#                         print(\"running WSI_preds:\",WSI_preds,\"\\t\",\"---------------------\")\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "#                 print(\"preds:\",preds,\"type:\",type(preds))\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "#             print(phase,\"OneEoch:\",WSI_preds)\n",
    "            Final_WSI_preds = [n.index(max(n)) for n in WSI_preds]\n",
    "#             print(phase,\"Final_WSI_preds:\",torch.tensor(Final_WSI_preds))\n",
    "#             print(phase,\"torch.tensor(WSI_label[phase]).double():\",torch.tensor(WSI_label[phase]).double())\n",
    "#             print(\"torch.tensor(Final_WSI_preds) == torch.tensor(WSI_label[phase]):\",torch.tensor(Final_WSI_preds) == torch.tensor(WSI_label[phase]))\n",
    "#             print(\"sum:\",torch.sum(torch.tensor(Final_WSI_preds) == torch.tensor(WSI_label[phase]).double()))\n",
    "#             print(\"len(Final_WSI_preds):\",len(Final_WSI_preds))\n",
    "            WSI_Acc = torch.sum(torch.tensor(Final_WSI_preds) == torch.tensor(WSI_label[phase])).double()/len(Final_WSI_preds)\n",
    "#             print(\"epoch_acc:\",epoch_acc,type(epoch_acc))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            print('{} WSI Acc: {:.4f}'.format(phase, WSI_Acc))\n",
    "            writer.add_scalar(phase+'/Loss', epoch_loss, epoch)\n",
    "            writer.add_scalar(phase+'/Accuracy', epoch_acc, epoch)\n",
    "            writer.add_scalar(phase+'/WSI Accuracy', WSI_Acc, epoch)\n",
    "            writer.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.5860 Acc: 0.6653\n",
      "train WSI Acc: 0.8077\n",
      "val Loss: 0.4316 Acc: 0.8235\n",
      "val WSI Acc: 0.9091\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.4308 Acc: 0.8367\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.2636 Acc: 0.9281\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2722 Acc: 0.9469\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.2105 Acc: 0.9412\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2635 Acc: 0.9020\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1904 Acc: 0.9346\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1961 Acc: 0.9429\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1761 Acc: 0.9412\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1656 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1690 Acc: 0.9412\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9429\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1637 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1293 Acc: 0.9633\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1629 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1776 Acc: 0.9102\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1635 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1456 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1659 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1514 Acc: 0.9469\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1630 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1244 Acc: 0.9592\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1626 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1364 Acc: 0.9592\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1640 Acc: 0.9542\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1686 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1630 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1213 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1653 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1207 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1673 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1132 Acc: 0.9673\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1673 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1140 Acc: 0.9796\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1672 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1542 Acc: 0.9469\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1667 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1317 Acc: 0.9551\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1659 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1458 Acc: 0.9469\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1647 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9510\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1637 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1136 Acc: 0.9633\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1658 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1229 Acc: 0.9714\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1664 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9510\n",
      "train WSI Acc: 1.0000\n",
      "val Loss: 0.1656 Acc: 0.9477\n",
      "val WSI Acc: 1.0000\n",
      "\n",
      "Training complete in 31m 16s\n",
      "Best val Acc: 0.954248\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
